{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetSbKRrczhD"
   },
   "source": [
    "Copyright 2020 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "HaswJZMq9B3c",
    "tags": []
   },
   "source": [
    "# @title Download Model {form-width: \"25%\"}\n",
    "\n",
    "%mkdir tapnet/checkpoints\n",
    "\n",
    "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/tapir_checkpoint.npy\n",
    "\n",
    "%ls tapnet/checkpoints"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "pip install --upgrade \"jax[cpu]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FxlHY242m-6Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Imports {form-width: \"25%\"}\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import tree\n",
    "\n",
    "from tapnet import tapir_model\n",
    "from tapnet.utils import transforms\n",
    "from tapnet.utils import viz_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7rfy2yobnHqw"
   },
   "outputs": [],
   "source": [
    "# @title Load Checkpoint {form-width: \"25%\"}\n",
    "\n",
    "checkpoint_path = 'tapnet/checkpoints/tapir_checkpoint.npy'\n",
    "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
    "params, state = ckpt_state['params'], ckpt_state['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "I7wOMJoSQzq1"
   },
   "outputs": [],
   "source": [
    "# @title Build Model {form-width: \"25%\"}\n",
    "\n",
    "def build_model(frames, query_points):\n",
    "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
    "  model = tapir_model.TAPIR()\n",
    "  outputs = model(\n",
    "      video=frames,\n",
    "      is_training=False,\n",
    "      query_points=query_points,\n",
    "      query_chunk_size=64,\n",
    "  )\n",
    "  return outputs\n",
    "\n",
    "model = hk.transform_with_state(build_model)\n",
    "model_apply = jax.jit(model.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ogRTRVgfSq0W"
   },
   "outputs": [],
   "source": [
    "# @title Utility Functions {form-width: \"25%\"}\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "  \"\"\"Preprocess frames to model inputs.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "\n",
    "  Returns:\n",
    "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
    "  \"\"\"\n",
    "  frames = frames.astype(np.float32)\n",
    "  frames = frames / 255 * 2 - 1\n",
    "  return frames\n",
    "\n",
    "\n",
    "def postprocess_occlusions(occlusions, expected_dist):\n",
    "  \"\"\"Postprocess occlusions to boolean visible flag.\n",
    "\n",
    "  Args:\n",
    "    occlusions: [num_points, num_frames], [-inf, inf], np.float32\n",
    "    expected_dist: [num_points, num_frames], [-inf, inf], np.float32\n",
    "\n",
    "  Returns:\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  # visibles = occlusions < 0\n",
    "  visibles = (1 - jax.nn.sigmoid(occlusions)) * (1 - jax.nn.sigmoid(expected_dist)) > 0.5\n",
    "  return visibles\n",
    "\n",
    "def inference(frames, query_points):\n",
    "  \"\"\"Inference on one video.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "    query_points: [num_points, 3], [0, num_frames/height/width], [t, y, x]\n",
    "\n",
    "  Returns:\n",
    "    tracks: [num_points, 3], [-1, 1], [t, y, x]\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  # Preprocess video to match model inputs format\n",
    "  frames = preprocess_frames(frames)\n",
    "  num_frames, height, width = frames.shape[0:3]\n",
    "  query_points = query_points.astype(np.float32)\n",
    "  frames, query_points = frames[None], query_points[None]  # Add batch dimension\n",
    "\n",
    "  # Model inference\n",
    "  rng = jax.random.PRNGKey(42)\n",
    "  outputs, _ = model_apply(params, state, rng, frames, query_points)\n",
    "  outputs = tree.map_structure(lambda x: np.array(x[0]), outputs)\n",
    "  tracks, occlusions, expected_dist = outputs['tracks'], outputs['occlusion'], outputs['expected_dist']\n",
    "\n",
    "  # Binarize occlusions\n",
    "  visibles = postprocess_occlusions(occlusions, expected_dist)\n",
    "  return tracks, visibles\n",
    "\n",
    "\n",
    "def sample_random_points(frame_max_idx, height, width, num_points):\n",
    "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
    "  y = np.random.randint(0, height, (num_points, 1))\n",
    "  x = np.random.randint(0, width, (num_points, 1))\n",
    "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
    "  points = np.concatenate((t, y, x), axis=-1).astype(np.int32)  # [num_points, 3]\n",
    "  return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Y1HkuvyT6SJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'tapnet/examplar_videos': File exists\n",
      "--2023-07-11 14:29:57--  https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.63.128, 142.251.163.128, 142.251.16.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.63.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 467706 (457K) [video/mp4]\n",
      "Saving to: 'tapnet/examplar_videos/horsejump-high.mp4.10'\n",
      "\n",
      "horsejump-high.mp4. 100%[===================>] 456.74K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2023-07-11 14:29:57 (7.73 MB/s) - 'tapnet/examplar_videos/horsejump-high.mp4.10' saved [467706/467706]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
    "\n",
    "%mkdir tapnet/examplar_videos\n",
    "\n",
    "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
    "\n",
    "video = media.read_video('tapnet/examplar_videos/horsejump-high.mp4')\n",
    "# media.show_video(video, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastplotlib as fpl\n",
    "from ipywidgets import IntSlider, VBox, Layout\n",
    "from sidecar import Sidecar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 480, 854, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_resized = np.zeros((50, 160, 284, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(video.shape[0]):\n",
    "    video_resized[i] = resize(video[i], (160, 284, 3), preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c960d60939e4946bc4edc15c9bd24f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot = fpl.Plot(size=(700, 400))\n",
    "\n",
    "plot.add_image(video_resized[0])\n",
    "\n",
    "def update_frame(change):\n",
    "    ix = change[\"new\"]\n",
    "    plot.graphics[0].data = video_resized[ix]\n",
    "\n",
    "slider = IntSlider(min=0, max=video_resized.shape[0] - 1, step=1, value=0)\n",
    "slider.observe(update_frame, \"value\")\n",
    "slider.layout = Layout(width=\"700px\")\n",
    "\n",
    "with Sidecar(title=\"draw\"):\n",
    "    display(VBox([plot.show(), slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot.camera.world.scale_y *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a polygon around the horse and rider by clicking on the polygon tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sample random points within the polygon\n",
    "vertices = plot.selectors[0].get_vertices()\n",
    "\n",
    "# returns boolean mask\n",
    "mask = polygon2mask(video.shape[1:-1], vertices).T\n",
    "\n",
    "# get points\n",
    "pts = np.argwhere(mask).astype(np.float32)\n",
    "\n",
    "# random sample of points\n",
    "n_query = 100\n",
    "ixs = np.random.choice(range(pts.shape[0]), n_query, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushalk/repos/fastplotlib/fastplotlib/graphics/_features/_base.py:34: UserWarning: converting float64 array to float32\n",
      "  warn(f\"converting {array.dtype} array to float32\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7f4ecc388b80 to ScatterGraphic at 0x7f4ecc4a5f90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.add_scatter(np.fliplr(pts[ixs]), sizes=10, colors=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rIASK5A2Rj0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: output size is not a multiple of 8. Final layer will round size down.\n"
     ]
    }
   ],
   "source": [
    "query_points = np.column_stack([np.zeros(n_query), pts[ixs]]).astype(np.int32)\n",
    "tracks, visibles = inference(video_resized, query_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b634bdb01fd745e68d3b82d9a0190cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tracks = fpl.Plot(size=(700, 400))\n",
    "\n",
    "plot_tracks.add_image(video_resized[0])\n",
    "\n",
    "pos0 = np.vstack([tracks[i][0] for i in range(len(tracks))])\n",
    "\n",
    "plot_tracks.add_scatter(\n",
    "    pos0, \n",
    "    cmap=\"jet\",\n",
    "    sizes=5, \n",
    "    name=\"pts\"\n",
    ")\n",
    "\n",
    "def update_frame(change):\n",
    "    frame_ix = change[\"new\"]\n",
    "    plot_tracks.graphics[0].data = video_resized[frame_ix]\n",
    "    \n",
    "    for i in range(len(tracks)):\n",
    "        plot_tracks[\"pts\"].data[i] = tracks[i][frame_ix]\n",
    "\n",
    "slider = IntSlider(min=0, max=video_resized.shape[0] - 1, step=1, value=0)\n",
    "slider.observe(update_frame, \"value\")\n",
    "slider.layout = Layout(width=\"700px\")\n",
    "\n",
    "with Sidecar(title=\"tracks\"):\n",
    "    display(VBox([plot_tracks.show(), slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tracks.camera.world.scale_y *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a gaussian filer to smooth the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks_filt = list()\n",
    "for t in tracks:\n",
    "    # gaussian filter xs and ys\n",
    "    tracks_filt.append(\n",
    "        np.column_stack(\n",
    "            [\n",
    "                gaussian_filter1d(t[:, 0], 1.5), # filter x vals\n",
    "                gaussian_filter1d(t[:, 1], 1.5)  # filter y vals\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tracks.add_line_collection(\n",
    "    tracks_filt, \n",
    "    cmap=\"jet\", # same cmap, colors will match\n",
    "    thickness=2,\n",
    "    name=\"tracks\"\n",
    ")\n",
    "\n",
    "# bring our points up so they're more visible\n",
    "plot_tracks[\"pts\"].position_z = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x and y components of velocity and accerlation\n",
    "\n",
    "x_velocity = list()\n",
    "y_velocity = list()\n",
    "\n",
    "x_accel = list()\n",
    "y_accel = list()\n",
    "\n",
    "for t in tracks_filt:\n",
    "    x_vel = np.gradient(t[:, 0])\n",
    "    x_velocity.append(x_vel)\n",
    "    \n",
    "    y_vel = np.gradient(t[:, 1])\n",
    "    y_velocity.append(y_vel)\n",
    "    \n",
    "    x_acc = np.gradient(x_vel)\n",
    "    x_accel.append(x_acc)\n",
    "    \n",
    "    y_acc = np.gradient(y_vel)\n",
    "    y_accel.append(y_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, y_velocity):\n",
    "    # velcoity is directional, so diverging colormaps!\n",
    "    \n",
    "    # blue = negative, down\n",
    "    # white = 0\n",
    "    # red = positive, up\n",
    "    \n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, y_accel):\n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, x_accel):\n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('/home/clewis7/Desktop/tapvid_davis/tapvid_davis.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_vids = [\"paragliding-launch\", \"kite-surf\", \"drift-chicane\",\n",
    "                            \"dance-twirl\", \"dog\", \"scooter-black\",\n",
    "                            \"car-roundabout\", \"soapbox\", \"breakdance\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41a2b5031b46b285f04a8c0aee1a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gridplot = fpl.GridPlot(shape=(3,3), size=(700, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for vid, subplot in zip(selected_vids, gridplot):\n",
    "    subplot.add_image(data[vid][\"video\"][0], name=vid)\n",
    "    subplot.camera.world.scale_y *= -1\n",
    "    \n",
    "def update_frame(change):\n",
    "    ix = change[\"new\"]\n",
    "    for vid, subplot in zip(selected_vids, gridplot):\n",
    "        subplot[vid].data = data[vid][\"video\"][ix]\n",
    "\n",
    "# length shortest vid is 43\n",
    "slider = IntSlider(min=0, max=43 - 1, step=1, value=0)\n",
    "slider.observe(update_frame, \"value\")\n",
    "slider.layout = Layout(width=\"700px\")\n",
    "\n",
    "with Sidecar(title=\"draw\"):\n",
    "    display(VBox([gridplot.show(), slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subplot in gridplot:\n",
    "    subplot.camera.world.scale_y *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
