{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fetSbKRrczhD"
   },
   "source": [
    "Copyright 2020 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaswJZMq9B3c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'tapnet/checkpoints': File exists\n",
      "--2023-07-11 02:29:31--  https://storage.googleapis.com/dm-tapnet/tapir_checkpoint.npy\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 172.253.62.128, 142.250.31.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n"
     ]
    }
   ],
   "source": [
    "# @title Download Model {form-width: \"25%\"}\n",
    "\n",
    "%mkdir tapnet/checkpoints\n",
    "\n",
    "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/tapir_checkpoint.npy\n",
    "\n",
    "%ls tapnet/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax[cpu] in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (0.4.13)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (from jax[cpu]) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (from jax[cpu]) (1.25.1)\n",
      "Requirement already satisfied: opt-einsum in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (from jax[cpu]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (from jax[cpu]) (1.11.1)\n",
      "Requirement already satisfied: jaxlib==0.4.13 in /home/kushalk/python-venvs/tapir/lib/python3.11/site-packages (from jax[cpu]) (0.4.13+cuda12.cudnn89)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade \"jax[cpu]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FxlHY242m-6Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Imports {form-width: \"25%\"}\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import tree\n",
    "\n",
    "from tapnet import tapir_model\n",
    "from tapnet.utils import transforms\n",
    "from tapnet.utils import viz_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7rfy2yobnHqw"
   },
   "outputs": [],
   "source": [
    "# @title Load Checkpoint {form-width: \"25%\"}\n",
    "\n",
    "checkpoint_path = 'tapnet/checkpoints/tapir_checkpoint.npy'\n",
    "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
    "params, state = ckpt_state['params'], ckpt_state['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I7wOMJoSQzq1"
   },
   "outputs": [],
   "source": [
    "# @title Build Model {form-width: \"25%\"}\n",
    "\n",
    "def build_model(frames, query_points):\n",
    "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
    "  model = tapir_model.TAPIR()\n",
    "  outputs = model(\n",
    "      video=frames,\n",
    "      is_training=False,\n",
    "      query_points=query_points,\n",
    "      query_chunk_size=64,\n",
    "  )\n",
    "  return outputs\n",
    "\n",
    "model = hk.transform_with_state(build_model)\n",
    "model_apply = jax.jit(model.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ogRTRVgfSq0W"
   },
   "outputs": [],
   "source": [
    "# @title Utility Functions {form-width: \"25%\"}\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "  \"\"\"Preprocess frames to model inputs.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "\n",
    "  Returns:\n",
    "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
    "  \"\"\"\n",
    "  frames = frames.astype(np.float32)\n",
    "  frames = frames / 255 * 2 - 1\n",
    "  return frames\n",
    "\n",
    "\n",
    "def postprocess_occlusions(occlusions, expected_dist):\n",
    "  \"\"\"Postprocess occlusions to boolean visible flag.\n",
    "\n",
    "  Args:\n",
    "    occlusions: [num_points, num_frames], [-inf, inf], np.float32\n",
    "    expected_dist: [num_points, num_frames], [-inf, inf], np.float32\n",
    "\n",
    "  Returns:\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  # visibles = occlusions < 0\n",
    "  visibles = (1 - jax.nn.sigmoid(occlusions)) * (1 - jax.nn.sigmoid(expected_dist)) > 0.5\n",
    "  return visibles\n",
    "\n",
    "def inference(frames, query_points):\n",
    "  \"\"\"Inference on one video.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "    query_points: [num_points, 3], [0, num_frames/height/width], [t, y, x]\n",
    "\n",
    "  Returns:\n",
    "    tracks: [num_points, 3], [-1, 1], [t, y, x]\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  # Preprocess video to match model inputs format\n",
    "  frames = preprocess_frames(frames)\n",
    "  num_frames, height, width = frames.shape[0:3]\n",
    "  query_points = query_points.astype(np.float32)\n",
    "  frames, query_points = frames[None], query_points[None]  # Add batch dimension\n",
    "\n",
    "  # Model inference\n",
    "  rng = jax.random.PRNGKey(42)\n",
    "  outputs, _ = model_apply(params, state, rng, frames, query_points)\n",
    "  outputs = tree.map_structure(lambda x: np.array(x[0]), outputs)\n",
    "  tracks, occlusions, expected_dist = outputs['tracks'], outputs['occlusion'], outputs['expected_dist']\n",
    "\n",
    "  # Binarize occlusions\n",
    "  visibles = postprocess_occlusions(occlusions, expected_dist)\n",
    "  return tracks, visibles\n",
    "\n",
    "\n",
    "def sample_random_points(frame_max_idx, height, width, num_points):\n",
    "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
    "  y = np.random.randint(0, height, (num_points, 1))\n",
    "  x = np.random.randint(0, width, (num_points, 1))\n",
    "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
    "  points = np.concatenate((t, y, x), axis=-1).astype(np.int32)  # [num_points, 3]\n",
    "  return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y1HkuvyT6SJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'tapnet/examplar_videos': File exists\n",
      "--2023-07-11 07:31:18--  https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.163.128, 172.253.62.128, 142.251.16.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.163.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 467706 (457K) [video/mp4]\n",
      "Saving to: 'tapnet/examplar_videos/horsejump-high.mp4.6'\n",
      "\n",
      "horsejump-high.mp4. 100%[===================>] 456.74K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-07-11 07:31:18 (4.31 MB/s) - 'tapnet/examplar_videos/horsejump-high.mp4.6' saved [467706/467706]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
    "\n",
    "%mkdir tapnet/examplar_videos\n",
    "\n",
    "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
    "\n",
    "video = media.read_video('tapnet/examplar_videos/horsejump-high.mp4')\n",
    "# media.show_video(video, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fastplotlib as fpl\n",
    "from ipywidgets import IntSlider, VBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 480, 854, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_resized = np.zeros((50, 160, 284, 3), dtype=np.uint8)\n",
    "\n",
    "for i in range(video.shape[0]):\n",
    "    video_resized[i] = resize(video[i], (160, 284, 3), preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7385611cc528440da938dd06c935fa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4728c3079e4bd982df224587a4c37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(JupyterWgpuCanvas(css_height='400px', css_width='700px'), HBox(children=(Button(…"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot = fpl.Plot(size=(700, 400))\n",
    "\n",
    "plot.add_image(video_resized[0])\n",
    "\n",
    "def update_frame(change):\n",
    "    ix = change[\"new\"]\n",
    "    plot.graphics[0].data = video_resized[ix]\n",
    "\n",
    "slider = IntSlider(min=0, max=video_resized.shape[0] - 1, step=1, value=0)\n",
    "slider.observe(update_frame, \"value\")\n",
    "\n",
    "VBox([plot.show(), slider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot.camera.world.scale_y *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a polygon around the horse and rider by clicking on the polygon tool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.draw import polygon2mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertices = plot.selectors[0].get_vertices()\n",
    "\n",
    "# returns boolean mask\n",
    "mask = polygon2mask(video.shape[1:-1], vertices).T\n",
    "\n",
    "# get points\n",
    "pts = np.argwhere(mask).astype(np.float32)\n",
    "\n",
    "# random sample of points\n",
    "n_query = 100\n",
    "ixs = np.random.choice(range(pts.shape[0]), n_query, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weakproxy at 0x7f61dc6a9f80 to ScatterGraphic at 0x7f6278201cd0>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.add_scatter(np.fliplr(pts[ixs]), sizes=10, colors=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "rIASK5A2Rj0X"
   },
   "outputs": [],
   "source": [
    "query_points = np.column_stack([np.zeros(n_query), pts[ixs]]).astype(np.int32)\n",
    "tracks, visibles = inference(video_resized, query_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88836e41804c42c89bc453824b451955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RFBOutputContext()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9999b04d4f4c5a9b949a926798651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(JupyterWgpuCanvas(css_height='400px', css_width='700px'), HBox(children=(Button(…"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_tracks = fpl.Plot(size=(700, 400))\n",
    "\n",
    "plot_tracks.add_image(video_resized[0])\n",
    "\n",
    "pos0 = np.vstack([tracks[i][0] for i in range(len(tracks))])\n",
    "\n",
    "plot_tracks.add_scatter(\n",
    "    pos0, \n",
    "    cmap=\"jet\",\n",
    "    sizes=5, \n",
    "    name=\"pts\"\n",
    ")\n",
    "\n",
    "def update_frame(change):\n",
    "    frame_ix = change[\"new\"]\n",
    "    plot_tracks.graphics[0].data = video_resized[frame_ix]\n",
    "    \n",
    "    for i in range(len(tracks)):\n",
    "        plot_tracks[\"pts\"].data[i] = tracks[i][frame_ix]\n",
    "\n",
    "slider = IntSlider(min=0, max=video_resized.shape[0] - 1, step=1, value=0)\n",
    "slider.observe(update_frame, \"value\")\n",
    "\n",
    "VBox([plot_tracks.show(), slider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tracks.camera.world.scale_y *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a gaussian filer to smooth the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tracks_filt = list()\n",
    "for t in tracks:\n",
    "    # gaussian filter xs and ys\n",
    "    tracks_filt.append(\n",
    "        np.column_stack(\n",
    "            [\n",
    "                gaussian_filter1d(t[:, 0], 1.5), # filter x vals\n",
    "                gaussian_filter1d(t[:, 1], 1.5)  # filter y vals\n",
    "            ]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tracks.add_line_collection(\n",
    "    tracks_filt, \n",
    "    cmap=\"jet\", # same cmap, colors will match\n",
    "    thickness=2,\n",
    "    name=\"tracks\"\n",
    ")\n",
    "\n",
    "# bring our points up so they're more visible\n",
    "plot_tracks[\"pts\"].position_z = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x and y components of velocity and accerlation\n",
    "\n",
    "x_velocity = list()\n",
    "y_velocity = list()\n",
    "\n",
    "x_accel = list()\n",
    "y_accel = list()\n",
    "\n",
    "for t in tracks_filt:\n",
    "    x_vel = np.gradient(t[:, 0])\n",
    "    x_velocity.append(x_vel)\n",
    "    \n",
    "    y_vel = np.gradient(t[:, 1])\n",
    "    y_velocity.append(y_vel)\n",
    "    \n",
    "    x_acc = np.gradient(x_vel)\n",
    "    x_accel.append(x_acc)\n",
    "    \n",
    "    y_acc = np.gradient(y_vel)\n",
    "    y_accel.append(y_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, y_velocity):\n",
    "    # velcoity is directional, so diverging colormaps!\n",
    "    \n",
    "    # blue = negative, down\n",
    "    # white = 0\n",
    "    # red = positive, up\n",
    "    \n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, y_accel):\n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for track, vals in zip(plot_tracks[\"tracks\"].graphics, x_accel):\n",
    "    track.cmap = \"bwr_r\"\n",
    "    track.cmap.values = vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "last_runtime": {
    "build_target": "//learning/grp/tools/ml_python:ml_notebook",
    "kind": "private"
   },
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
